{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF 8215 - Intelligence artif.: méthodes et algorithmes \n",
    "## Automne 2018 - TP3 - Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date de rendu: 6 Décembre**\n",
    "\n",
    "**Fichiers à rendre:**\n",
    "    * TP3_FR.ipynb complété\n",
    "    * SoftmaxClassifier.py complété\n",
    "    * test_prediction.csv le fichier de résultat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Le but de ce TP est de vous donner un aperçu du déroulement général d'un projet de machine learning tout en vous familiarisant avec des librairies python adaptées.\n",
    "\n",
    "\n",
    "Dans la première partie, vous implémenterez un algorithme de classification multiclasse appelé **softmax regression** à l'aide uniquement de la bibliothèque **numpy** et l'intégrerez à la bibliothèque **scikit-learn**.\n",
    "\n",
    "Dans la deuxième partie, vous prendrez connaissance du **dataset** utilisé pour ce projet. Et vous serez amenés à effectuer le **preprocessing** de ces données pour qu'elles soient utilisables dans les algorithmes de machine learning classiques. Vous utiliserez les bibliothèques **pandas** et **scikit-learn**.\n",
    "\n",
    "Enfin, dans la troisième partie, vous comparerez l'efficacité du modèle que vous avez implémenté avec d'autres modèles déjà implémentés dans **sklearn**. Puis vous tenterez d'améliorer les performances de l'algorithme sélectionné.\n",
    "\n",
    "Pour enfin soumettre vos résultats sur la plateforme **kaggle**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Pour installer **pandas** et **scikit-learn** le plus simple est de télécharger et d'installer **Anaconda** qui regroupe les packages les plus utilisés pour le calcul scientifique et la science des données.\n",
    "\n",
    "Vous trouverez la distribution ici : https://www.anaconda.com/download/#linux .\n",
    "\n",
    "Assurez-vous d'avoir la version **20.0** de **scikit-learn**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 1: Compétition (2 points)\n",
    "\n",
    "Quand vous aurez terminé le TP, vous pourrez soumettre vos prédictions sur **kaggle**, vous obtiendrez votre performance en terme de **log loss**.\n",
    "Vous pouvez ensuite me communiquer ce résultat par mail (laurent.boucaud@polymtl.ca) et me joindre votre fichier de prédiction sur l'ensemble de test(pour vérification).\n",
    "\n",
    "Une conversation dans le forum sera créée pour tenir à jour le meilleur score obtenu par une des équipes du cours.\n",
    "\n",
    "Tant qu'aucun forum n'est créé, **ne m'envoyez pas vos performances si elles sont supérieures à 0.8 de log loss**.\n",
    "\n",
    "Une fois le premier meilleur score affiché dans le forum, **ne me communiquez vos résultats que si votre log loss est inférieure au précédent meilleur score**.\n",
    "\n",
    "Le nombre de points obtenus sera proportionnel au classement des équipes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Softmax Regression (10 points)\n",
    "\n",
    "Dans cette partie vous implémenterez **softmax regression** la variante de **logistic regression** qui permet d'effectuer de la classification pour un nombre de classe supérieur à 2.\n",
    "\n",
    "Le code à compléter se trouve dans le fichier **SoftmaxClassifier.py**. \n",
    "\n",
    "**Pour cet exercice, la contrainte est d'utiliser uniquement la bibliothèque numpy**\n",
    "\n",
    "## Encapsulation avec sklearn\n",
    "\n",
    "La classe **SoftmaxClassifier** hérite des classes **BaseEstimator** et **ClassifierMixin** de **scikit-learn** ce qui nous permettra d'utiliser facilement avec notre classifier les outils fournis par scikit-learn dans la suite du TP.\n",
    "\n",
    "Pour la compatibilité, le classifier implémente obligatoirement les méthodes:\n",
    "\n",
    "* **fit**: responsable de l'entraînement du modèle\n",
    "* **predict_proba**: permet de prédire la probabilité de chaque classe pour chaque exemple du dataset fourni.\n",
    "* **predict**: permet de prédire la classe pour chaque exemple du dataset fourni.\n",
    "* **score**: permet de quantifier l'écart entre les classes prédites et les classes réelles pour le dataset fourni\n",
    "\n",
    "\n",
    "## Train/Test set:\n",
    "\n",
    "Quand on veut tester les performances de l'apprentissage d'un algorithme de machine learning, on **ne le teste pas sur les données utilisées pour l'apprentissage**.\n",
    "\n",
    "En effet, ce qui nous intéresse c'est que notre algorithme soit **capable de généraliser** ses prédictions à des données qu'il n'a **jamais vu**.\n",
    "\n",
    "Pour illustrer, si on teste un algorithme sur les données d'entrainement, on teste sa capacité à **apprendre par coeur** le dataset et non à **généraliser**.\n",
    "\n",
    "Par conséquent, quand on reçoit un nouveau dataset, la première chose à faire et de le **diviser en deux parties**: un ensemble d'**entraînement** (**70-80%** du dataset) et un ensemble de **test**(**20-30%** du dataset).\n",
    "\n",
    "Tous les algorithmes de **traitement des données** et d'apprentissage devront être appris uniquement sur l'ensemble d'entraînement et appliqués ensuite sur l'ensemble de test.\n",
    "\n",
    "Cela garantit l'absence de connaissances préalables de l'ensemble de test lors de l'entrainement.\n",
    "\n",
    "## Gradient descent\n",
    "\n",
    "La descente de gradient est un algorithme qui permet trouver la solution optimale d'un certains nombre de problèmes. Le principe est le suivant: on définit une **fonction de coût J**  qui caractérise le problème.\n",
    "Cette fonction dépend d'un ensemble de **paramètres $\\theta$ **. La descente de gradient cherche à **minimiser** la fonction de coût en **modifiant itérativement** les paramètres.\n",
    "\n",
    "### Gradient\n",
    "\n",
    "Le gradient de la fonction de coûts pour un $\\theta$ donné, correspond à la direction dans laquelle il faut modifier $\\theta$ pour réduire la valeur de la fonction de coût. \n",
    "\n",
    "La fonction de coût est minimale quand le gradient est nul.\n",
    "\n",
    "Concrètement, on initialize $\\theta$ aléatoirement, et on effectue à chaque itération un pas pour réduire la fonction de coût jusqu'à convergence de l'algorithme à un minimum.\n",
    "\n",
    "### Learning rate\n",
    "\n",
    "Le taux d'apprentissage correspond à la taille du pas que l'on va effectuer dans la direction du gradient.\n",
    "Plus il est grand, plus la convergence est rapide mais il y a un risque que l'algorithme diverge.\n",
    "\n",
    "Plus il est petit, plus la convergence est lente.\n",
    "\n",
    "### Batch gradient descent\n",
    "\n",
    "Il existe plusieurs algorithmes de descente de gradient. Nous utiliserons Batch gradient descent.\n",
    "\n",
    "Dans cet algorithme, avant de mettre à jour $\\theta$, on calcule les gradients sur l'ensemble des exemples d'entraînement.\n",
    "\n",
    "### Epoch\n",
    "\n",
    "Il s'agit d'un pas de la descente de gradient, soit une unique mise à jour de gradient.\n",
    "\n",
    "### Bias/Variance tradeoff\n",
    "\n",
    "Lorsqu'on entraine un algorithme de machine learning on cherche un équilibre entre **biais** et **variance**.\n",
    "\n",
    "Un modèle avec un **biais fort**, est un modèle qui est **trop simple** pour la structure donnée considérée (modèle linéaire pour données quadratiques), cela limite la capacité du modèle à généraliser. On appelle aussi le biais **underfitting**.\n",
    "\n",
    "Un modèle avec une **variance élevée** signifie qu'il est sensible aux petites variations dans les données d'entrainement, cela correspond à l'**overfitting**, c'est-à-dire que le modèle est trop proche de la structure de l'ensemble d'entrainement ce qui **limite sa capacité à généraliser**.\n",
    "\n",
    "Un modèle avec un **biais important** aura une **mauvaise performance** sur l'ensemble d'**entraînement**.\n",
    "Un modèle avec une **variance importante** aura une performance bien **moins bonne** sur l'ensemble de **test** que sur l'ensemble d'**entrainement**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot-encoding\n",
    "\n",
    "En machine learning pour représenter un vecteur de données catégoriques, on utilise le one-hot encoding.\n",
    "\n",
    "Pour un vecteur comportant 5 exemples et 3 catégories différentes, on le représente sous forme d'une matrice de taille 5 par 3. Cette matrice est entièrement remplie de 0 sauf à l'indice correspondant au numéro de la classe pour chaque exemple.\n",
    "\n",
    "\n",
    "Par exemple\n",
    "$ y = \\left(\\begin{array}{cc} \n",
    "1 \\\\\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "2 \\\\\n",
    "\\end{array}\\right) $\n",
    "\n",
    "devient:\n",
    "\n",
    "$ yohe =  \\left(\\begin{array}{cc} \n",
    "1. & 0. & 0.\\\\\n",
    "1. & 0. & 0.\\\\\n",
    "0. & 1. & 0.\\\\\n",
    "0. & 0. & 1.\\\\\n",
    "0. & 1. & 0.\\\\\n",
    "\\end{array}\\right) $\n",
    "\n",
    "\n",
    "#### Question 1 (1 point)\n",
    "Implémentez  la fonction  **_one_hot**  dans SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import SoftmaxClassifier as sc\n",
    "import numpy as np\n",
    "\n",
    "sc = sc.SoftmaxClassifier()\n",
    "\n",
    "y = np.array([0,10,3,7])\n",
    "\n",
    "y_one_hot = sc._one_hot(y)\n",
    "\n",
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice de poids\n",
    "\n",
    "Soit $ X_{m * n} $ la matrice d'exemple et $ \\Theta _{n*K} $ la matrice de poids avec:\n",
    "\n",
    "* **m** le nombre d'exemples\n",
    "* **n** le nombre de features\n",
    "* **k** le nombre de classes\n",
    "\n",
    "Il est d'usage d'ajouter une colonne supplémentaire à X, cette colonne est remplie de 1. Pour prendre en compte ce changement, il faut rajouter une ligne à la matrice $\\Theta$.\n",
    "\n",
    "On obtient X_bias$_{m*(n+1)}$ et $ \\Theta _{(n+1)*K} $\n",
    "\n",
    "\n",
    "Intuitivement, à chaque classe K est associée une colonne de $\\theta$.\n",
    "\n",
    "On note $\\theta_k$ le vecteur de dimension n+1 la colonne de poids associée à la prédiction de la classe k.\n",
    "\n",
    "$\\Theta$ = [$\\theta_0$,$\\theta_1$... $\\theta_k$ ... $\\theta_n$ ]\n",
    "\n",
    "Ainsi $ z = x * \\Theta $ donne un vecteur de dimension K qui correspond aux **logits** associés à x pour chacune des classes.\n",
    "\n",
    "#### Question 2 (1 point)\n",
    "Dans la fonction  **fit**  dans SoftmaxClassifier.py instanciez X_bias et initialisez $\\Theta$ aléatoirement. (ligne 74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.81710425 0.55131217]\n",
      " [0.75869146 0.07995923]\n",
      " [0.51521158 0.82199287]\n",
      " [0.05681981 0.5277036 ]]\n"
     ]
    }
   ],
   "source": [
    "import SoftmaxClassifier as sc\n",
    "import numpy as np\n",
    "\n",
    "sc = sc.SoftmaxClassifier()\n",
    "\n",
    "X = np.array([1, 1, 2, 5, 2, 10]).reshape(2,3)\n",
    "\n",
    "# on fait des classes arbitraires\n",
    "y = np.array([1, 2])\n",
    "\n",
    "print(sc.fit(X, y).theta_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "On veut convertir le vecteur de logits **z** obtenu dans la partie précédente, en un **vecteur de probabilité**.\n",
    "\n",
    "Pour cela on définit la **fonction softmax**:\n",
    "\n",
    "$$ \\hat{p_x}^k = softmax(z)_k = \\frac{exp(z_k)}{\\sum_{\\substack{1<j<K}} exp(z_j)} $$\n",
    "\n",
    "Intuitivement, pour un logit de z, $z_k$, on prend l'exponentielle de cette valeur et on la divise par la somme des exponentielles de chaque logit du vecteur **z**. On obtient  $\\hat{p_x}^k$ la probabilité que l'exemple **x** appartienne à la classe **k**.\n",
    "\n",
    "On réitère l'opération pour chaque logit du vecteur **z**. \n",
    "\n",
    "On obtient ainsi un vecteur de probabilités $\\hat{p_x}$ pour un exemple **x**. \n",
    "\n",
    "La division permet de rendre la somme des termes du vecteur $\\hat{p_x}$ égale à 1 ce qui est indispensable dans le cadre des probabilités.\n",
    "\n",
    "#### Question 3 (1 point)\n",
    "Implémentez  la fonction  **_softmax**  dans SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [2 5]]\n",
      "[[0.5        0.5       ]\n",
      " [0.04742587 0.95257413]]\n"
     ]
    }
   ],
   "source": [
    "import SoftmaxClassifier as sc\n",
    "import numpy as np\n",
    "\n",
    "sc = sc.SoftmaxClassifier()\n",
    "z = np.array([1, 1, 2, 5]).reshape(2,2)\n",
    "\n",
    "print(z)\n",
    "\n",
    "p= sc._softmax(z)\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 (1 point)\n",
    "En utilisant la fonction **_softmax** de la question 3, implémentez  les fonctions  **predict_proba** et **predict**  dans SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 3.90172467e-36]\n",
      " [1.36936729e-08 9.99999986e-01]]\n",
      "[0 1]\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import SoftmaxClassifier as sc\n",
    "import numpy as np\n",
    "\n",
    "sc = sc.SoftmaxClassifier()\n",
    "\n",
    "X = np.array([9, 1, 2, -1, 4, 2]).reshape(2,3)\n",
    "\n",
    "# on fait des classes arbitraires\n",
    "y = np.transpose(np.array([1, 2]))\n",
    "\n",
    "sc.fit(X,y)\n",
    "\n",
    "print(sc.predict_proba(X,y))\n",
    "\n",
    "pred = sc.predict(X,y)\n",
    "\n",
    "print(pred)\n",
    "\n",
    "print(sc._one_hot(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction de coût Log loss\n",
    "\n",
    "Soit la fonction de coût log loss (ou cross entropy):\n",
    "\n",
    "$$ J( \\Theta) = \\frac{-1}{m}\\sum_{\\substack{1<i<m}} \\sum_{\\substack{1<k<K}} y_k^i log( \\hat{p_k}^i ) $$\n",
    "\n",
    "avec:\n",
    "* **K** le nombre de classes\n",
    "* **m** le nombre d'exemples dans les données\n",
    "* $ \\hat{p_k}^i  $  la probabilité que l'exemple i soit de la classe k\n",
    "* $y_k^i$ vaut 1 si la classe cible de l'exemple i est k, 0 sinon\n",
    "\n",
    "**Détail d'implémentation:** La fonction n'est pas définie pour des valeurs de probabilité de 0. ou 1., il faut donc s'assurer que étant donné $\\epsilon$, les probabilités sont comprises dans [$\\epsilon$, 1. - $\\epsilon$].\n",
    "#### Question 5 (1 point)\n",
    "Implémentez  la fonction  **_cost_function**  dans SoftmaxClassifier.py en prenant en compte le **détail d'implémentation** (variable self.eps) et utilisez-la pour calculer la variable **loss** dans la fonction **fit** (ligne 84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.18731707101499\n"
     ]
    }
   ],
   "source": [
    "import SoftmaxClassifier as sc\n",
    "import numpy as np\n",
    "\n",
    "sc = sc.SoftmaxClassifier()\n",
    "\n",
    "X = np.array([9, 1, 2, -1, 4, 2]).reshape(2,3)\n",
    "\n",
    "# on fait des classes arbitraires\n",
    "y = np.transpose(np.array([0, 1]))\n",
    "\n",
    "sc.fit(X,y)\n",
    "\n",
    "p = sc.predict_proba(X,y)\n",
    "\n",
    "J = sc._cost_function(p, y)\n",
    "\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient de la fonction de coût\n",
    "\n",
    "Le **gradient de J** par rapport à la classe k (par rapport à $\\theta_k$) est :\n",
    "\n",
    "\n",
    "$$ \\Delta_{\\theta_k}J( \\Theta) = \\frac{1}{m} \\sum_{\\substack{1<i<m}}( \\hat{p_k}^i - y_k^i)x^i  $$\n",
    "\n",
    "avec:\n",
    "* **K** le nombre de classes\n",
    "* **m** le nombre d'exemples dans les données\n",
    "* $ \\hat{p_k}^i  $  la probabilité que l'exemple i soit de la classe k\n",
    "* $y_k^i$ vaut 1 si la classe cible de l'exemple i est k, 0 sinon\n",
    "\n",
    "Sous **forme matricielle**, on peut écrire le **gradient de J par rapport à $\\Theta$**:\n",
    "$$ \\Delta_J( \\Theta) = \\frac{1}{m} X_{bias}^T *( \\hat{p} - y_{ohe}) $$\n",
    "\n",
    "avec:\n",
    "* $\\hat{p}$ la matrice de probabilité prédite pour chaque example et pour chaque classe\n",
    "* $y_{ohe}$ la version one-hot de y\n",
    "* $X_{bias}^T$  la matrice transposée de $X_{bias}$\n",
    "* **\\*** le produit matriciel\n",
    "\n",
    "#### Question 6 (1 point)\n",
    "Implémentez  la fonction  **_get_gradient**  dans SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.576542208721685\n",
      "1.358795384226926\n",
      "0.9050974973697725\n",
      "0.6620477244023208\n",
      "0.5130054858173803\n",
      "0.41892057134713523\n",
      "0.35874919346796597\n",
      "0.32000492926975227\n",
      "0.29492493997583924\n",
      "0.2785848463800079\n",
      "0.2678407852874205\n",
      "0.2606820276719561\n",
      "0.2558219111740264\n",
      "0.2524369631620713\n",
      "0.2500001108367357\n",
      "0.24817396936544067\n",
      "0.24674255325034733\n",
      "0.2455675730890004\n",
      "0.24456046581586194\n",
      "0.24366449411495092\n",
      "0.24284329004515595\n",
      "0.24207352294925435\n",
      "0.24134020690201086\n",
      "0.24063369746297303\n",
      "0.239947769584415\n",
      "0.23927838745879815\n",
      "0.23862291720774959\n",
      "0.23797962298983297\n",
      "0.23734734449656125\n",
      "0.2367252905370941\n",
      "0.23611290691989872\n",
      "0.23550979188466886\n",
      "0.23491564196660747\n",
      "0.23433021733761977\n",
      "0.23375331961291806\n",
      "0.23318477763568285\n",
      "0.23262443836786545\n",
      "0.23207216104910455\n",
      "0.23152781344741852\n",
      "0.230991269448811\n",
      "0.23046240750395774\n",
      "0.22994110962359773\n",
      "0.22942726072526634\n",
      "0.22892074820505334\n",
      "0.22842146165354466\n",
      "0.22792929266420117\n",
      "0.22744413470105973\n",
      "0.22696588300455617\n",
      "0.22649443452190365\n",
      "0.22602968785333571\n",
      "0.22557154320865608\n",
      "0.22511990237053214\n",
      "0.22467466866224714\n",
      "0.2242357469184531\n",
      "0.22380304345798305\n",
      "0.22337646605811895\n",
      "0.22295592392993102\n",
      "0.22254132769443302\n",
      "0.2221325893593954\n",
      "0.22172962229670742\n",
      "0.22133234122021964\n",
      "0.22094066216401875\n",
      "0.22055450246110442\n",
      "0.2201737807224438\n",
      "0.21979841681638834\n",
      "0.219428331848441\n",
      "0.21906344814136275\n",
      "0.2187036892156103\n",
      "0.21834897977010007\n",
      "0.2179992456632881\n",
      "0.21765441389456458\n",
      "0.21731441258595346\n",
      "0.2169791709641145\n",
      "0.21664861934264118\n",
      "0.21632268910465077\n",
      "0.21600131268566006\n",
      "0.2156844235567436\n",
      "0.2153719562079689\n",
      "0.215063846132104\n",
      "0.21476002980859416\n",
      "0.21446044468780057\n",
      "0.2141650291755\n",
      "0.21387372261763854\n",
      "0.21358646528533703\n",
      "0.21330319836014194\n",
      "0.2130238639195202\n",
      "0.21274840492259206\n",
      "0.21247676519609854\n",
      "0.21220888942059962\n",
      "0.2119447231169002\n",
      "0.21168421263269754\n",
      "0.2114273051294503\n",
      "0.21117394856946176\n",
      "0.21092409170317578\n",
      "0.21067768405668236\n",
      "0.21043467591942613\n",
      "0.21019501833212045\n",
      "0.2099586630748555\n",
      "0.20972556265540512\n",
      "0.20949567029772392\n",
      "0.20926893993063284\n",
      "0.20904532617669158\n",
      "0.2088247843412523\n",
      "0.20860727040169297\n",
      "0.20839274099682759\n",
      "0.20818115341648974\n",
      "0.207972465591286\n",
      "0.20776663608251786\n",
      "0.20756362407226878\n",
      "0.20736338935365245\n",
      "0.20716589232122085\n",
      "0.20697109396152968\n",
      "0.20677895584385747\n",
      "0.2065894401110756\n",
      "0.206402509470669\n",
      "0.20621812718590155\n",
      "0.20603625706712714\n",
      "0.20585686346324125\n",
      "0.20567991125327245\n",
      "0.2055053658381107\n",
      "0.20533319313237008\n",
      "0.20516335955638498\n",
      "0.20499583202833474\n",
      "0.20483057795649778\n",
      "0.20466756523163127\n",
      "0.20450676221947373\n",
      "0.20434813775337118\n",
      "0.2041916611270203\n",
      "0.2040373020873324\n",
      "0.20388503082741005\n",
      "0.20373481797963935\n",
      "0.20358663460889415\n",
      "0.20344045220584847\n",
      "0.2032962426803992\n",
      "0.20315397835519372\n",
      "0.20301363195926297\n",
      "0.20287517662175614\n",
      "0.20273858586577775\n",
      "0.20260383360232387\n",
      "0.20247089412431502\n",
      "0.20233974210072647\n",
      "0.20221035257081238\n",
      "0.20208270093842304\n",
      "0.2019567629664135\n",
      "0.201832514771142\n",
      "0.20170993281705646\n",
      "0.2015889939113687\n",
      "0.20146967519881298\n",
      "0.20135195415648982\n",
      "0.20123580858879042\n",
      "0.20112121662240512\n",
      "0.20100815670140965\n",
      "0.20089660758242983\n",
      "0.2007865483298851\n",
      "0.20067795831130686\n",
      "0.20057081719273162\n",
      "0.20046510493416758\n",
      "0.20036080178513402\n",
      "0.20025788828027108\n",
      "0.2001563452350194\n",
      "0.2000561537413693\n",
      "0.19995729516367683\n",
      "0.19985975113454535\n",
      "0.1997635035507744\n",
      "0.1996685345693724\n",
      "0.19957482660363055\n",
      "0.19948236231926142\n",
      "0.19939112463059738\n",
      "0.19930109669684923\n",
      "0.19921226191842434\n",
      "0.19912460393330225\n",
      "0.19903810661346857\n",
      "0.198952754061404\n",
      "0.1988685306066288\n",
      "0.19878542080230305\n",
      "0.19870340942187845\n",
      "0.19862248145580375\n",
      "0.19854262210828222\n",
      "0.1984638167940796\n",
      "0.19838605113538163\n",
      "0.19830931095870258\n",
      "0.19823358229183985\n",
      "0.1981588513608779\n",
      "0.19808510458723877\n",
      "0.19801232858477824\n",
      "0.19794051015692776\n",
      "0.19786963629388096\n",
      "0.19779969416982346\n",
      "0.1977306711402068\n",
      "0.19766255473906424\n",
      "0.19759533267636784\n",
      "0.19752899283542835\n",
      "0.19746352327033312\n",
      "0.1973989122034262\n",
      "0.19733514802282495\n",
      "0.19727221927997812\n",
      "0.19721011468725902\n",
      "0.19714882311559734\n",
      "0.19708833359214795\n",
      "0.19702863529799436\n",
      "0.1969697175658887\n",
      "0.19691156987802633\n",
      "0.19685418186385512\n",
      "0.19679754329791688\n",
      "0.1967416440977247\n",
      "0.1966864743216704\n",
      "0.19663202416696546\n",
      "0.19657828396761345\n",
      "0.19652524419241357\n",
      "0.1964728954429944\n",
      "0.19642122845187734\n",
      "0.19637023408057241\n",
      "0.196319903317699\n",
      "0.19627022727713886\n",
      "0.19622119719621528\n",
      "0.19617280443390117\n",
      "0.19612504046905374\n",
      "0.19607789689867686\n",
      "0.19603136543620864\n",
      "0.19598543790983622\n",
      "0.1959401062608362\n",
      "0.19589536254193918\n",
      "0.1958511989157199\n",
      "0.19580760765301247\n",
      "0.19576458113134748\n",
      "0.1957221118334155\n",
      "0.1956801923455519\n",
      "0.19563881535624483\n",
      "0.19559797365466677\n",
      "0.1955576601292267\n",
      "0.19551786776614624\n",
      "0.19547858964805437\n",
      "0.19543981895260595\n",
      "0.19540154895111972\n",
      "0.19536377300723734\n",
      "0.19532648457560164\n",
      "0.1952896772005559\n",
      "0.19525334451486206\n",
      "0.1952174802384374\n",
      "0.1951820781771127\n",
      "0.19514713222140498\n",
      "0.1951126363453129\n",
      "0.19507858460512623\n",
      "0.19504497113825656\n",
      "0.19501179016208273\n",
      "0.19497903597281524\n",
      "0.19494670294437674\n",
      "0.19491478552729893\n",
      "0.19488327824763718\n",
      "0.1948521757058996\n",
      "0.19482147257599364\n",
      "0.19479116360418613\n",
      "0.19476124360808195\n",
      "0.19473170747561447\n",
      "0.1947025501640533\n",
      "0.19467376669902564\n",
      "0.19464535217355194\n",
      "0.19461730174709746\n",
      "0.1945896106446346\n",
      "0.19456227415572275\n",
      "0.19453528763359929\n",
      "0.19450864649428462\n",
      "0.19448234621570074\n",
      "0.19445638233680157\n",
      "0.19443075045671848\n",
      "0.19440544623391504\n",
      "0.19438046538535725\n",
      "0.1943558036856941\n",
      "0.19433145696645043\n",
      "0.19430742111523266\n",
      "0.19428369207494428\n",
      "0.19426026584301426\n",
      "0.19423713847063667\n",
      "0.19421430606202048\n",
      "0.19419176477365097\n",
      "0.19416951081356176\n",
      "0.19414754044061805\n",
      "0.19412584996380827\n",
      "0.19410443574154918\n",
      "0.19408329418099785\n",
      "0.19406242173737578\n",
      "0.19404181491330258\n",
      "0.19402147025813749\n",
      "0.19400138436733358\n",
      "0.1939815538817981\n",
      "0.19396197548726424\n",
      "0.19394264591367089\n",
      "0.1939235619345523\n",
      "0.1939047203664351\n",
      "0.19388611806824543\n",
      "0.19386775194072414\n",
      "0.1938496189258499\n",
      "0.19383171600627153\n",
      "0.1938140402047473\n",
      "0.1937965885835936\n",
      "0.19377935824414094\n",
      "0.1937623463261973\n",
      "0.19374555000751972\n",
      "0.19372896650329346\n",
      "0.19371259306561864\n",
      "0.1936964269830036\n",
      "0.19368046557986643\n",
      "0.19366470621604293\n",
      "0.19364914628630192\n",
      "0.1936337832198673\n",
      "0.19361861447994705\n",
      "0.19360363756326887\n",
      "0.1935888499996226\n",
      "0.19357424935140888\n",
      "0.1935598332131942\n",
      "0.19354559921127243\n",
      "0.19353154500323286\n",
      "0.19351766827753392\n",
      "0.19350396675308304\n",
      "0.19349043817882217\n",
      "0.19347708033332042\n",
      "0.19346389102437042\n",
      "0.1934508680885925\n",
      "0.19343800939104294\n",
      "0.19342531282482806\n",
      "0.19341277631072493\n",
      "0.19340039779680557\n",
      "0.19338817525806745\n",
      "0.19337610669606942\n",
      "0.1933641901385728\n",
      "0.19335242363918614\n",
      "0.19334080527701658\n",
      "0.1933293331563256\n",
      "0.19331800540618943\n",
      "0.19330682018016399\n",
      "0.1932957756559554\n",
      "0.19328487003509412\n",
      "0.19327410154261385\n",
      "0.19326346842673625\n",
      "0.19325296895855765\n",
      "0.19324260143174188\n",
      "0.1932323641622175\n",
      "0.19322225548787827\n",
      "0.19321227376828798\n",
      "0.1932024173843899\n",
      "0.19319268473822027\n",
      "0.19318307425262554\n",
      "0.19317358437098273\n",
      "0.19316421355692526\n",
      "0.19315496029407184\n",
      "0.19314582308575867\n",
      "0.1931368004547758\n",
      "0.19312789094310767\n",
      "0.1931190931116758\n",
      "0.19311040554008688\n",
      "0.19310182682638233\n",
      "0.1930933555867938\n",
      "0.19308499045549932\n",
      "0.19307673008438475\n",
      "0.19306857314280795\n",
      "0.19306051831736565\n",
      "0.19305256431166506\n",
      "0.19304470984609656\n",
      "0.19303695365761128\n",
      "0.19302929449950035\n",
      "0.19302173114117877\n",
      "0.19301426236797034\n",
      "0.19300688698089752\n",
      "0.1929996037964726\n",
      "0.19299241164649247\n",
      "0.1929853093778362\n",
      "0.19297829585226517\n",
      "0.19297136994622605\n",
      "0.19296453055065646\n",
      "0.19295777657079316\n",
      "0.19295110692598333\n",
      "0.19294452054949812\n",
      "0.19293801638834807\n",
      "0.19293159340310212\n",
      "0.19292525056770932\n",
      "0.19291898686932094\n",
      "0.19291280130811767\n",
      "0.19290669289713688\n",
      "0.19290066066210432\n",
      "0.19289470364126587\n",
      "0.1928888208852232\n",
      "0.19288301145677178\n",
      "0.1928772744307398\n",
      "0.1928716088938299\n",
      "0.19286601394446343\n",
      "0.1928604886926269\n",
      "0.1928550322597195\n",
      "0.19284964377840325\n",
      "0.19284432239245597\n",
      "0.19283906725662475\n",
      "0.1928338775364826\n",
      "0.19282875240828662\n",
      "0.19282369105883734\n",
      "0.19281869268534194\n",
      "0.19281375649527713\n",
      "0.19280888170625513\n",
      "0.19280406754589083\n",
      "0.19279931325167132\n",
      "0.1927946180708272\n",
      "0.19278998126020438\n",
      "0.19278540208613976\n",
      "0.19278087982433645\n",
      "0.19277641375974178\n",
      "0.19277200318642673\n",
      "0.1927676474074678\n",
      "0.19276334573482812\n",
      "0.19275909748924308\n",
      "0.19275490200010534\n",
      "0.1927507586053525\n",
      "0.19274666665135587\n",
      "0.19274262549281063\n",
      "0.19273863449262765\n",
      "0.19273469302182689\n",
      "0.1927308004594318\n",
      "0.1927269561923658\n",
      "0.19272315961534897\n",
      "0.19271941013079782\n",
      "0.19271570714872469\n",
      "0.19271205008663958\n",
      "0.19270843836945298\n",
      "0.19270487142937945\n",
      "0.19270134870584424\n",
      "0.19269786964538865\n",
      "0.19269443370157807\n",
      "0.19269104033491208\n",
      "0.1926876890127337\n",
      "0.19268437920914128\n",
      "0.19268111040490107\n",
      "0.19267788208736136\n",
      "0.1926746937503671\n",
      "0.19267154489417632\n",
      "0.19266843502537687\n",
      "0.1926653636568053\n",
      "0.19266233030746582\n",
      "0.19265933450245093\n",
      "0.19265637577286304\n",
      "0.19265345365573683\n",
      "0.19265056769396272\n",
      "0.19264771743621173\n",
      "0.1926449024368615\n",
      "0.1926421222559211\n",
      "0.1926393764589606\n",
      "0.19263666461703888\n",
      "0.19263398630663214\n",
      "0.1926313411095653\n",
      "0.19262872861294336\n",
      "0.19262614840908263\n",
      "0.19262360009544438\n",
      "0.19262108327456917\n",
      "0.19261859755401084\n",
      "0.19261614254627274\n",
      "0.19261371786874415\n",
      "0.19261132314363733\n",
      "0.19260895799792627\n",
      "0.1926066220632851\n",
      "0.19260431497602828\n",
      "0.19260203637705064\n",
      "0.19259978591176974\n",
      "0.19259756323006666\n",
      "0.19259536798623011\n",
      "0.19259319983889928\n",
      "0.19259105845100827\n",
      "0.1925889434897315\n",
      "0.1925868546264295\n",
      "0.19258479153659505\n",
      "0.19258275389980067\n",
      "0.1925807413996465\n",
      "0.1925787537237089\n",
      "0.1925767905634898\n",
      "0.19257485161436597\n",
      "0.19257293657554075\n",
      "0.1925710451499939\n",
      "0.1925691770444348\n",
      "0.1925673319692539\n",
      "0.19256550963847652\n",
      "0.1925637097697155\n",
      "0.1925619320841271\n",
      "0.1925601763063644\n",
      "0.19255844216453394\n",
      "0.19255672939015073\n",
      "0.19255503771809543\n",
      "0.19255336688657199\n",
      "0.19255171663706455\n",
      "0.19255008671429621\n",
      "0.19254847686618748\n",
      "0.19254688684381682\n",
      "0.1925453164013788\n",
      "0.19254376529614559\n",
      "0.19254223328842784\n",
      "0.19254072014153606\n",
      "0.19253922562174233\n",
      "0.19253774949824273\n",
      "0.19253629154312085\n",
      "0.19253485153131017\n",
      "0.19253342924055877\n",
      "0.19253202445139334\n",
      "0.19253063694708428\n",
      "0.1925292665136099\n",
      "0.19252791293962412\n",
      "0.19252657601642018\n",
      "0.19252525553789918\n",
      "0.19252395130053604\n",
      "0.19252266310334742\n",
      "0.19252139074785907\n",
      "0.19252013403807477\n",
      "0.19251889278044437\n",
      "0.19251766678383309\n",
      "0.1925164558594914\n",
      "0.19251525982102435\n",
      "0.19251407848436192\n",
      "0.1925129116677297\n",
      "0.19251175919162022\n",
      "0.19251062087876383\n",
      "0.19250949655410088\n",
      "0.1925083860447534\n",
      "0.1925072891799979\n",
      "0.1925062057912385\n",
      "0.19250513571197939\n",
      "0.19250407877779854\n",
      "0.1925030348263222\n",
      "0.19250200369719808\n",
      "0.19250098523207076\n",
      "0.19249997927455625\n",
      "0.19249898567021678\n",
      "0.1924980042665369\n",
      "0.1924970349128989\n",
      "0.19249607746055908\n",
      "0.19249513176262392\n",
      "0.1924941976740271\n",
      "0.19249327505150682\n",
      "0.1924923637535818\n",
      "0.192491463640531\n",
      "0.19249057457436924\n",
      "0.19248969641882707\n",
      "0.1924888290393283\n",
      "0.19248797230296924\n",
      "0.19248712607849705\n",
      "0.19248629023629005\n",
      "0.19248546464833605\n",
      "0.19248464918821284\n",
      "0.19248384373106842\n",
      "0.19248304815360076\n",
      "0.19248226233403837\n",
      "0.19248148615212185\n",
      "0.1924807194890839\n",
      "0.19247996222763136\n",
      "0.1924792142519262\n",
      "0.19247847544756805\n",
      "0.1924777457015749\n",
      "0.19247702490236707\n",
      "0.1924763129397482\n",
      "0.19247560970488797\n",
      "0.1924749150903064\n",
      "0.19247422898985517\n",
      "0.19247355129870217\n",
      "0.19247288191331452\n",
      "0.19247222073144252\n",
      "0.19247156765210305\n",
      "0.1924709225755648\n",
      "0.19247028540333144\n",
      "0.19246965603812694\n",
      "0.1924690343838802\n",
      "0.19246842034570955\n",
      "0.19246781382990857\n",
      "0.1924672147439307\n",
      "0.1924666229963754\n",
      "0.19246603849697327\n",
      "0.19246546115657234\n",
      "0.19246489088712376\n",
      "0.1924643276016683\n",
      "0.19246377121432273\n",
      "0.19246322164026652\n",
      "0.19246267879572765\n",
      "0.19246214259797118\n",
      "0.19246161296528455\n",
      "0.19246108981696633\n",
      "0.19246057307331263\n",
      "0.19246006265560484\n",
      "0.19245955848609772\n",
      "0.19245906048800682\n",
      "0.19245856858549698\n",
      "0.1924580827036699\n",
      "0.1924576027685531\n",
      "0.19245712870708787\n",
      "0.19245666044711787\n",
      "0.19245619791737872\n",
      "0.19245574104748586\n",
      "0.1924552897679241\n",
      "0.19245484401003662\n",
      "0.19245440370601455\n",
      "0.19245396878888638\n",
      "0.19245353919250685\n",
      "0.19245311485154765\n",
      "0.19245269570148704\n",
      "0.19245228167859896\n",
      "0.19245187271994446\n",
      "0.19245146876336094\n",
      "0.19245106974745252\n",
      "0.1924506756115813\n",
      "0.19245028629585703\n",
      "0.19244990174112853\n",
      "0.19244952188897363\n",
      "0.1924491466816914\n",
      "0.19244877606229194\n",
      "0.1924484099744882\n",
      "0.1924480483626872\n",
      "0.19244769117198107\n",
      "0.19244733834813915\n",
      "0.19244698983759895\n",
      "0.1924466455874585\n",
      "0.19244630554546704\n",
      "0.19244596966001853\n",
      "0.19244563788014213\n",
      "0.19244531015549554\n",
      "0.19244498643635644\n",
      "0.19244466667361487\n",
      "0.19244435081876587\n",
      "0.192444038823902\n",
      "0.19244373064170553\n",
      "0.1924434262254418\n",
      "0.19244312552895151\n",
      "0.1924428285066433\n",
      "0.19244253511348774\n",
      "0.19244224530500892\n",
      "0.1924419590372794\n",
      "0.19244167626691153\n",
      "0.1924413969510519\n",
      "0.1924411210473747\n",
      "0.19244084851407495\n",
      "0.1924405793098617\n",
      "0.19244031339395218\n",
      "0.19244005072606546\n",
      "0.19243979126641608\n",
      "0.19243953497570793\n",
      "0.19243928181512815\n",
      "0.19243903174634128\n",
      "0.19243878473148357\n",
      "0.19243854073315636\n",
      "0.19243829971442125\n",
      "0.19243806163879373\n",
      "0.19243782647023794\n",
      "0.1924375941731611\n",
      "0.1924373647124079\n",
      "0.19243713805325513\n",
      "0.19243691416140618\n",
      "0.19243669300298605\n",
      "0.19243647454453622\n",
      "0.1924362587530089\n",
      "0.1924360455957629\n",
      "0.1924358350405572\n",
      "0.1924356270555479\n",
      "0.19243542160928162\n",
      "0.1924352186706914\n",
      "0.1924350182090922\n",
      "0.19243482019417554\n",
      "0.19243462459600497\n",
      "0.19243443138501226\n",
      "0.19243424053199187\n",
      "0.1924340520080967\n",
      "0.19243386578483443\n",
      "0.19243368183406176\n",
      "0.19243350012798172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1924333206391376\n",
      "0.19243314334041062\n",
      "0.19243296820501443\n",
      "0.19243279520649118\n",
      "0.19243262431870836\n",
      "0.19243245551585345\n",
      "0.19243228877243113\n",
      "0.19243212406325866\n",
      "0.19243196136346258\n",
      "0.19243180064847418\n",
      "0.19243164189402645\n",
      "0.19243148507614993\n",
      "0.1924313301711691\n",
      "0.19243117715569902\n",
      "0.1924310260066417\n",
      "0.19243087670118172\n",
      "0.19243072921678422\n",
      "0.19243058353119025\n",
      "0.19243043962241407\n",
      "0.1924302974687392\n",
      "0.1924301570487156\n",
      "0.1924300183411562\n",
      "0.19242988132513367\n",
      "0.19242974597997703\n",
      "0.19242961228526884\n",
      "0.1924294802208421\n",
      "0.19242934976677645\n",
      "0.19242922090339615\n",
      "0.19242909361126642\n",
      "0.19242896787119063\n",
      "0.19242884366420726\n",
      "0.19242872097158736\n",
      "0.1924285997748314\n",
      "0.19242848005566637\n",
      "0.1924283617960431\n",
      "0.19242824497813338\n",
      "0.192428129584328\n",
      "0.19242801559723297\n",
      "0.1924279029996674\n",
      "0.19242779177466068\n",
      "0.1924276819054505\n",
      "0.1924275733754794\n",
      "0.1924274661683928\n",
      "0.19242736026803636\n",
      "0.19242725565845376\n",
      "0.1924271523238837\n",
      "0.19242705024875828\n",
      "0.19242694941769972\n",
      "0.19242684981551889\n",
      "0.19242675142721222\n",
      "0.19242665423796004\n",
      "0.19242655823312427\n",
      "0.19242646339824557\n",
      "0.19242636971904187\n",
      "0.1924262771814056\n",
      "0.19242618577140247\n",
      "0.19242609547526793\n",
      "0.1924260062794062\n",
      "0.192425918170388\n",
      "0.19242583113494832\n",
      "0.19242574515998423\n",
      "0.19242566023255323\n",
      "0.19242557633987137\n",
      "0.1924254934693107\n",
      "0.1924254116083982\n",
      "0.19242533074481286\n",
      "0.19242525086638507\n",
      "0.1924251719610935\n",
      "0.19242509401706398\n",
      "0.1924250170225679\n",
      "0.1924249409660196\n",
      "0.1924248658359755\n",
      "0.19242479162113157\n",
      "0.19242471831032254\n",
      "0.19242464589251926\n",
      "0.19242457435682742\n",
      "0.19242450369248618\n",
      "0.19242443388886615\n",
      "0.19242436493546772\n",
      "0.1924242968219199\n",
      "0.19242422953797816\n",
      "0.19242416307352378\n",
      "0.19242409741856104\n",
      "0.19242403256321686\n",
      "0.19242396849773874\n",
      "0.1924239052124932\n",
      "0.19242384269796486\n",
      "0.1924237809447541\n",
      "0.1924237199435766\n",
      "0.19242365968526154\n",
      "0.19242360016075\n",
      "0.19242354136109346\n",
      "0.19242348327745326\n",
      "0.1924234259010985\n",
      "0.19242336922340503\n",
      "0.19242331323585393\n",
      "0.19242325793003048\n",
      "0.19242320329762272\n",
      "0.1924231493304204\n",
      "0.1924230960203132\n",
      "0.19242304335929\n",
      "0.1924229913394378\n",
      "0.19242293995293996\n",
      "0.19242288919207565\n",
      "0.1924228390492178\n",
      "0.19242278951683295\n",
      "0.1924227405874796\n",
      "0.19242269225380676\n",
      "0.19242264450855381\n",
      "0.19242259734454828\n",
      "0.1924225507547055\n",
      "0.19242250473202716\n",
      "0.1924224592696007\n",
      "0.1924224143605978\n",
      "0.19242236999827333\n",
      "0.19242232617596494\n",
      "0.1924222828870912\n",
      "0.19242224012515108\n",
      "0.19242219788372325\n",
      "0.19242215615646432\n",
      "0.1924221149371089\n",
      "0.19242207421946758\n",
      "0.19242203399742638\n",
      "0.19242199426494666\n",
      "0.19242195501606274\n",
      "0.19242191624488242\n",
      "0.19242187794558466\n",
      "0.19242184011242022\n",
      "0.1924218027397096\n",
      "0.19242176582184256\n",
      "0.1924217293532775\n",
      "0.19242169332854042\n",
      "0.19242165774222414\n",
      "0.19242162258898737\n",
      "0.1924215878635541\n",
      "0.1924215535607126\n",
      "0.19242151967531484\n",
      "0.19242148620227548\n",
      "0.19242145313657122\n",
      "0.1924214204732403\n",
      "0.19242138820738092\n",
      "0.19242135633415194\n",
      "0.19242132484877053\n",
      "0.1924212937465124\n",
      "0.19242126302271115\n",
      "0.19242123267275715\n",
      "0.19242120269209684\n",
      "0.19242117307623235\n",
      "0.19242114382072056\n",
      "0.19242111492117278\n",
      "0.1924210863732538\n",
      "0.19242105817268085\n",
      "0.19242103031522412\n",
      "0.19242100279670465\n",
      "0.19242097561299512\n",
      "0.19242094876001792\n",
      "0.1924209222337456\n",
      "0.19242089603019968\n",
      "0.19242087014545017\n",
      "0.1924208445756151\n",
      "0.19242081931685956\n",
      "0.1924207943653958\n",
      "0.19242076971748207\n",
      "0.19242074536942222\n",
      "0.19242072131756505\n",
      "0.19242069755830404\n",
      "0.19242067408807664\n",
      "0.1924206509033638\n",
      "0.19242062800068907\n",
      "0.19242060537661868\n",
      "0.19242058302776055\n",
      "0.1924205609507641\n",
      "0.1924205391423195\n",
      "0.1924205175991574\n",
      "0.1924204963180478\n",
      "0.19242047529580097\n",
      "0.192420454529265\n",
      "0.19242043401532732\n",
      "0.19242041375091296\n",
      "0.19242039373298395\n",
      "0.19242037395854023\n",
      "0.1924203544246177\n",
      "0.19242033512828843\n",
      "0.19242031606666032\n",
      "0.19242029723687665\n",
      "0.19242027863611527\n",
      "0.19242026026158832\n",
      "0.19242024211054243\n",
      "0.192420224180257\n",
      "0.1924202064680453\n",
      "0.19242018897125313\n",
      "0.19242017168725817\n",
      "0.192420154613471\n",
      "0.19242013774733288\n",
      "0.1924201210863164\n",
      "0.1924201046279253\n",
      "0.1924200883696937\n",
      "0.19242007230918523\n",
      "0.19242005644399396\n",
      "0.19242004077174268\n",
      "0.19242002529008334\n",
      "0.1924200099966965\n",
      "0.19241999488929096\n",
      "0.19241997996560367\n",
      "0.19241996522339863\n",
      "0.19241995066046763\n",
      "0.1924199362746289\n",
      "0.1924199220637275\n",
      "0.19241990802563486\n",
      "0.19241989415824806\n",
      "0.19241988045948977\n",
      "0.19241986692730817\n",
      "0.1924198535596766\n",
      "0.19241984035459256\n",
      "0.19241982731007845\n",
      "0.19241981442418032\n",
      "0.19241980169496822\n",
      "0.1924197891205359\n",
      "0.19241977669899998\n",
      "0.1924197644285003\n",
      "0.192419752307199\n",
      "0.19241974033328124\n",
      "0.19241972850495348\n",
      "0.19241971682044445\n",
      "0.19241970527800478\n",
      "0.19241969387590557\n",
      "0.1924196826124398\n",
      "0.1924196714859206\n",
      "0.19241966049468212\n",
      "0.19241964963707853\n",
      "0.1924196389114841\n",
      "0.19241962831629264\n",
      "0.19241961784991812\n",
      "0.19241960751079346\n",
      "0.19241959729737046\n",
      "0.1924195872081201\n",
      "0.19241957724153191\n",
      "0.19241956739611357\n",
      "0.19241955767039132\n",
      "0.19241954806290917\n",
      "0.19241953857222854\n",
      "0.19241952919692895\n",
      "0.1924195199356068\n",
      "0.1924195107868757\n",
      "0.19241950174936606\n",
      "0.1924194928217251\n",
      "0.1924194840026163\n",
      "0.19241947529071973\n",
      "0.19241946668473128\n",
      "0.19241945818336284\n",
      "0.19241944978534187\n",
      "0.19241944148941137\n",
      "0.19241943329432942\n",
      "0.19241942519886987\n",
      "0.19241941720182076\n",
      "0.19241940930198523\n",
      "0.19241940149818096\n",
      "0.1924193937892401\n",
      "0.19241938617400875\n",
      "0.19241937865134723\n",
      "0.19241937122012998\n",
      "0.19241936387924463\n",
      "0.19241935662759244\n",
      "0.19241934946408856\n",
      "0.19241934238766087\n",
      "0.19241933539725026\n",
      "0.19241932849181081\n",
      "0.19241932167030912\n",
      "0.19241931493172454\n",
      "0.1924193082750485\n",
      "0.192419301699285\n",
      "0.19241929520345036\n",
      "0.19241928878657227\n",
      "0.1924192824476907\n",
      "0.19241927618585708\n",
      "0.1924192700001346\n",
      "0.19241926388959793\n",
      "0.19241925785333241\n",
      "0.19241925189043507\n",
      "0.19241924600001392\n",
      "0.19241924018118742\n",
      "0.19241923443308506\n",
      "0.192419228754847\n",
      "0.19241922314562349\n",
      "0.19241921760457564\n",
      "0.19241921213087432\n",
      "0.19241920672370078\n",
      "0.19241920138224583\n",
      "0.19241919610571068\n",
      "0.19241919089330584\n",
      "0.19241918574425143\n",
      "0.1924191806577774\n",
      "0.19241917563312283\n",
      "0.19241917066953615\n",
      "0.19241916576627455\n",
      "0.192419160922605\n",
      "0.19241915613780253\n",
      "0.19241915141115173\n",
      "0.19241914674194563\n",
      "0.19241914212948547\n",
      "0.1924191375730818\n",
      "0.19241913307205272\n",
      "0.19241912862572502\n",
      "0.19241912423343377\n",
      "0.19241911989452215\n",
      "0.1924191156083409\n",
      "0.19241911137424927\n",
      "0.1924191071916137\n",
      "0.19241910305980878\n",
      "0.19241909897821657\n",
      "0.19241909494622647\n",
      "0.19241909096323567\n",
      "0.1924190870286484\n",
      "0.1924190831418761\n",
      "0.19241907930233762\n",
      "0.19241907550945894\n",
      "0.19241907176267253\n",
      "0.19241906806141806\n",
      "0.19241906440514223\n",
      "0.1924190607932982\n",
      "0.19241905722534564\n",
      "0.19241905370075135\n",
      "0.19241905021898786\n",
      "0.1924190467795348\n",
      "0.19241904338187796\n",
      "0.19241904002550894\n",
      "0.192419036709926\n",
      "0.1924190334346333\n",
      "0.19241903019914108\n",
      "0.1924190270029657\n",
      "0.19241902384562912\n",
      "0.19241902072665926\n",
      "0.19241901764558964\n",
      "0.19241901460195973\n",
      "0.1924190115953142\n",
      "0.19241900862520378\n",
      "0.1924190056911843\n",
      "0.19241900279281715\n",
      "[[ 1.18085234e-04 -1.18085234e-04]\n",
      " [-7.20655541e-06  7.20655541e-06]\n",
      " [-1.15697779e-05  1.15697779e-05]\n",
      " [-7.42256460e-06  7.42256460e-06]]\n"
     ]
    }
   ],
   "source": [
    "import SoftmaxClassifier as sc\n",
    "import numpy as np\n",
    "\n",
    "sc = sc.SoftmaxClassifier(regularization = True)\n",
    "\n",
    "X = np.array([9, 1, 2, -1, 4, 2]).reshape(2,3)\n",
    "\n",
    "# on fait des classes arbitraires\n",
    "y = np.transpose(np.array([0, 1]))\n",
    "\n",
    "sc.fit(X,y)\n",
    "\n",
    "X_bias = np.c_[ np.ones(X.shape[0]), X ]\n",
    "p = sc.predict_proba(X,y)\n",
    "\n",
    "print(sc._get_gradient(X_bias, y, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mise à jour des poids\n",
    "\n",
    "Quand le gradient a été calculé, il faut mettre à jour les poids avec ces gradients.\n",
    "\n",
    "$$ \\Theta  = \\Theta - \\gamma \\Delta J( \\Theta) $$\n",
    "\n",
    "\n",
    "avec:\n",
    "* $\\Theta$ la matrice de poids\n",
    "* $\\gamma$  le taux d'apprentissage\n",
    "* $\\Delta J( \\Theta)$ le gradient de $J( \\Theta)$ selon $\\Theta$\n",
    "\n",
    "#### Question 7 (1 point)\n",
    "Mettez à jour la variable **self.theta_** dans la fonction **fit**  dans SoftmaxClassifier.py (ligne 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voir code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Pour limiter l'**overfitting**, on utilise la régularisation, il s'agit d'ajouter un terme à la fonction de coût $J( \\Theta)$.\n",
    "\n",
    "Ce terme va ajouter des contraintes sur les poids du modèle lors de l'entrainement.\n",
    "Nous allons utiliser la régularisation **L2** :\n",
    "\n",
    "$$ L2(\\Theta) = \\alpha \\sum_{\\substack{1<=i<n}} \\sum_{\\substack{0<=k<K}} \\theta_{i,k}^2 $$ \n",
    "\n",
    "avec:\n",
    "\n",
    "* $\\alpha$ le coefficient de régularisation\n",
    "\n",
    "**Remarque:** La première somme ne commence pas à 0 mais à 1 parce qu'on ne régularise pas les poids associés à la colonne de biais de X.\n",
    "\n",
    "Le fait d'ajouter ce terme conduit le modèle à apprendre les données tout en gardant ses poids le plus petit possible.\n",
    "\n",
    "\n",
    "\n",
    "#### Question 8 (1 point)\n",
    "Modifiez les fonctions  **_get_gradient** et **_cost_function** pour prendre en compte la régularisation lorsque le booléen self.regularization est vrai  dans SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157.0473502155248\n",
      "[[-4.99990125e-01  4.99990125e-01]\n",
      " [-4.70742767e+02 -1.44916839e+02]\n",
      " [-8.09465673e+02 -3.40008847e+02]\n",
      " [-2.10895756e+02 -4.66114008e+02]]\n"
     ]
    }
   ],
   "source": [
    "import SoftmaxClassifier as sc\n",
    "import numpy as np\n",
    "\n",
    "sc = sc.SoftmaxClassifier(regularization = True)\n",
    "\n",
    "X = np.array([9, 1, 2, -1, 4, 2]).reshape(2,3)\n",
    "\n",
    "# on fait des classes arbitraires\n",
    "y = np.transpose(np.array([0, 1]))\n",
    "\n",
    "sc.fit(X,y)\n",
    "\n",
    "X_bias = np.c_[ np.ones(X.shape[0]), X ]\n",
    "p = sc.predict_proba(X,y)\n",
    "\n",
    "print(np.array(sc._get_gradient(X_bias, y, p)))\n",
    "#print(sc._cost_function(y,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9 (1 point)\n",
    "\n",
    "Le terme de régularisation est utilisé uniquement pendant l'entraînement. Quand on veut évaluer la performance du modèle **après entrainement**, on utilise la fonction de coût **non-régulée**.\n",
    "\n",
    "Implémentez la fonction **score** qui permet d'évaluer la qualité de la prédiction **après entrainement** dans SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping\n",
    "\n",
    "Un trop grand nombre d'**epoch** peut résulter en **overfitting**.\n",
    "Pour pallier à ce problème, on peut utiliser le mécanisme d'**early stopping**.\n",
    "Il s'agit d'arrêter l'entraînement si la différence de la fonction de coût entre deux **epochs consécutives** est inférieure à un **seuil**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Question 10 (1 point)\n",
    "\n",
    "Finissez d'implémenter la fonction **fit** en y ajoutant le mécanisme d'**early stopping**  quand le booléen **self.early_stopping** est vrai le seuil est donné par la variable **self.threshold**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208.31762812551787\n",
      "[[-7.95208476e-14  7.94548463e-14]\n",
      " [-3.92878845e+02 -8.22440345e+02]\n",
      " [-7.59956374e+02 -3.01683847e+02]\n",
      " [-4.32102131e+02 -1.66249857e+02]]\n"
     ]
    }
   ],
   "source": [
    "import SoftmaxClassifier as sc\n",
    "import numpy as np\n",
    "\n",
    "sc = sc.SoftmaxClassifier(regularization = True, early_stopping=True)\n",
    "\n",
    "X = np.array([9, 1, 2, -1, 4, 2]).reshape(2,3)\n",
    "\n",
    "# on fait des classes arbitraires\n",
    "y = np.transpose(np.array([0, 1]))\n",
    "\n",
    "sc.fit(X,y)\n",
    "\n",
    "X_bias = np.c_[ np.ones(X.shape[0]), X ]\n",
    "p = sc.predict_proba(X,y)\n",
    "\n",
    "print(np.array(sc._get_gradient(X_bias, y, p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de la solution:\n",
    "\n",
    "Le code ci-dessous importe le dataset de classification multiclasse **iris** disponible sur sklearn. Les données sont divisées en deux parties, l'ensemble d'entraînement et l'ensemble de test, puis elles sont normalisées.\n",
    "\n",
    "Le classifier implémenté dans le fichier **SoftmaxClassifier.py** est importé puis entrainé sur l'ensemble d'entrainement et testé sur l'ensemble de test.\n",
    "\n",
    "Le but de cette partie est juste de vérifier votre implémentation **quand vous êtes sûrs que votre code fonctionne**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# load dataset\n",
    "data,target =load_iris().data,load_iris().target\n",
    "\n",
    "# split data in train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split( data, target, test_size=0.33, random_state=42)\n",
    "\n",
    "# standardize columns using normal distribution\n",
    "# fit on X_train and not on X_test to avoid Data Leakage\n",
    "s = StandardScaler()\n",
    "X_train = s.fit_transform(X_train)\n",
    "X_test = s.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SoftmaxClassifier import SoftmaxClassifier\n",
    "\n",
    "# import the custom classifier\n",
    "cl = SoftmaxClassifier()\n",
    "\n",
    "# train on X_train and not on X_test to avoid overfitting\n",
    "train_p = cl.fit_predict(X_train,y_train)\n",
    "test_p = cl.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous obtenez des valeurs relativement proches pour l'ensemble de test et d'entrainement, et qu'elles sont au moins supérieures à 0.8, votre modèle devrait être correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# display precision, recall and f1-score on train/test set\n",
    "print(\"train : \"+ str(precision_recall_fscore_support(y_train, train_p,average = \"macro\")))\n",
    "print(\"test : \"+ str(precision_recall_fscore_support(y_test, test_p,average = \"macro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(cl.losses_)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data preprocessing (8 points)\n",
    "\n",
    "##  Kaggle \n",
    "Kaggle est un site dédié au machine learning. On y retrouve un grand nombre de dataset.\n",
    "Des compétitions sont organisées par des organisations. Ces dernières fournissent un dataset et un objectif. Les \"kagglers\" qui participent à ces compétitions soumettent leurs résultats en ligne. Il y a souvent des prix ou des emplois pour ceux qui obtiennent les meilleurs résultats.\n",
    "\n",
    "Il s'agit d'un bon moyen pour développer ses compétences en machine learning sur des vrais datasets.\n",
    "\n",
    "Vous pouvez créer un compte si vous voulez comparer vos résultats à ceux déjà en ligne pour la dataset que nous allons étudier.\n",
    "\n",
    "Vous pouvez créer un compte ici: https://www.kaggle.com/\n",
    "\n",
    "\n",
    "## Austin Animal Center Shelter Animal Outcomes dataset\n",
    "Le dataset que nous utiliserons est le \"Animal Outcomes dataset\" disponible à l'adresse suivante: https://www.kaggle.com/c/shelter-animal-outcomes.\n",
    "\n",
    "Il s'agit d'un problème de **classification multiclasse** des animaux sont recueillis dans un refuge après avoir été abandonnés, le but est de prédire la manière dont ils vont \"quitter \" le lieu:\n",
    "* Adoption\n",
    "* Retour au propriétaire\n",
    "* Décès \n",
    "* Euthanasie\n",
    "* Transfert à un autre centre\n",
    "\n",
    "Pour plus d'informations sur les données, rendez-vous sur kaggle.\n",
    "\n",
    "## Déroulement d'un projet de machine learning\n",
    "\n",
    "Le but de la suite de ce TP est de vous faire étudier une version simplifiée d'un projet complet de machine learning:\n",
    "\n",
    "1. Nettoyage des données, traitement des valeurs manquantes\n",
    "2. Mise en forme des données pour pouvoir les utiliser dans les algorithmes de machine learning\n",
    "3. Feature engineering: transformation ou combinaison de features entre elles\n",
    "4. Comparaison des performances des différents choix effectués lors du traîtement des données\n",
    "5. Comparaison des performances de différents modèles (dont celui implémenté en première partie)\n",
    "6. Optimisation des hyper-paramètres\n",
    "\n",
    "## Scikit-learn\n",
    "http://scikit-learn.org/stable/\n",
    "\n",
    "Il s'agit d'une bibliothèque de machine learning et data mining, elle propose des outils pour l'analyse et le traîtement des données,  des algorithmes classiques de machine learning comme les réseaux de neuronnes, la régression logistique, les SVM ou autre, enfin des outils permettant de comparer les modèles entre eux comme la cross validation.\n",
    "\n",
    "## Pandas\n",
    "\n",
    "Une bibliothèque permettant de stocker des données et de les manipuler facilement\n",
    "\n",
    "Les deux éléments de base de pandas sont le dataframe et la serie.\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.html\n",
    "\n",
    "## Data processing tutorial\n",
    "\n",
    "**Avant de continuer le TP**, familiarisez-vous avec le **pré-traitement des données**, **pandas** et **scikit-learn**, un tutoriel est disponible dans le fichier: **data_processing_tutorial.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "#### Chargement de l'ensemble d'entraînement et de l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PATH = \"data/\"\n",
    "X_train = pd.read_csv(PATH + \"train.csv\")\n",
    "X_test = pd.read_csv(PATH + \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppression de colonnes inutiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns = [\"OutcomeSubtype\",\"AnimalID\"])\n",
    "X_test = X_test.drop(columns = [\"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train.drop(columns = [\"OutcomeType\"]),X_train[\"OutcomeType\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 premiers exemples de l'ensemble d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>AnimalType</th>\n",
       "      <th>SexuponOutcome</th>\n",
       "      <th>AgeuponOutcome</th>\n",
       "      <th>Breed</th>\n",
       "      <th>Color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hambone</td>\n",
       "      <td>2014-02-12 18:22:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>1 year</td>\n",
       "      <td>Shetland Sheepdog Mix</td>\n",
       "      <td>Brown/White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Emily</td>\n",
       "      <td>2013-10-13 12:44:00</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Spayed Female</td>\n",
       "      <td>1 year</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>Cream Tabby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pearce</td>\n",
       "      <td>2015-01-31 12:28:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Pit Bull Mix</td>\n",
       "      <td>Blue/White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-07-11 19:09:00</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>3 weeks</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>Blue Cream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-11-15 12:52:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Lhasa Apso/Miniature Poodle</td>\n",
       "      <td>Tan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name             DateTime AnimalType SexuponOutcome AgeuponOutcome  \\\n",
       "0  Hambone  2014-02-12 18:22:00        Dog  Neutered Male         1 year   \n",
       "1    Emily  2013-10-13 12:44:00        Cat  Spayed Female         1 year   \n",
       "2   Pearce  2015-01-31 12:28:00        Dog  Neutered Male        2 years   \n",
       "3      NaN  2014-07-11 19:09:00        Cat    Intact Male        3 weeks   \n",
       "4      NaN  2013-11-15 12:52:00        Dog  Neutered Male        2 years   \n",
       "\n",
       "                         Breed        Color  \n",
       "0        Shetland Sheepdog Mix  Brown/White  \n",
       "1       Domestic Shorthair Mix  Cream Tabby  \n",
       "2                 Pit Bull Mix   Blue/White  \n",
       "3       Domestic Shorthair Mix   Blue Cream  \n",
       "4  Lhasa Apso/Miniature Poodle          Tan  "
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 premiers exemples de l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>AnimalType</th>\n",
       "      <th>SexuponOutcome</th>\n",
       "      <th>AgeuponOutcome</th>\n",
       "      <th>Breed</th>\n",
       "      <th>Color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summer</td>\n",
       "      <td>2015-10-12 12:15:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Intact Female</td>\n",
       "      <td>10 months</td>\n",
       "      <td>Labrador Retriever Mix</td>\n",
       "      <td>Red/White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cheyenne</td>\n",
       "      <td>2014-07-26 17:59:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Spayed Female</td>\n",
       "      <td>2 years</td>\n",
       "      <td>German Shepherd/Siberian Husky</td>\n",
       "      <td>Black/Tan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gus</td>\n",
       "      <td>2016-01-13 12:20:00</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>1 year</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>Brown Tabby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pongo</td>\n",
       "      <td>2013-12-28 18:12:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>4 months</td>\n",
       "      <td>Collie Smooth Mix</td>\n",
       "      <td>Tricolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skooter</td>\n",
       "      <td>2015-09-24 17:59:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Miniature Poodle Mix</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Name             DateTime AnimalType SexuponOutcome AgeuponOutcome  \\\n",
       "0    Summer  2015-10-12 12:15:00        Dog  Intact Female      10 months   \n",
       "1  Cheyenne  2014-07-26 17:59:00        Dog  Spayed Female        2 years   \n",
       "2       Gus  2016-01-13 12:20:00        Cat  Neutered Male         1 year   \n",
       "3     Pongo  2013-12-28 18:12:00        Dog    Intact Male       4 months   \n",
       "4   Skooter  2015-09-24 17:59:00        Dog  Neutered Male        2 years   \n",
       "\n",
       "                            Breed        Color  \n",
       "0          Labrador Retriever Mix    Red/White  \n",
       "1  German Shepherd/Siberian Husky    Black/Tan  \n",
       "2          Domestic Shorthair Mix  Brown Tabby  \n",
       "3               Collie Smooth Mix     Tricolor  \n",
       "4            Miniature Poodle Mix        White  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 premiers exemples de l'attribut à prédire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Return_to_owner\n",
       "1         Euthanasia\n",
       "2           Adoption\n",
       "3           Transfer\n",
       "4           Transfer\n",
       "Name: OutcomeType, dtype: object"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travail demandé\n",
    "\n",
    "Pour vous faire gagner du temps, une partie des colonnes (Name,DateTime,color) ont déjà été traitées.\n",
    "\n",
    "\n",
    "En vous appuyant sur le tutoriel fourni, vous devez écrire un pipeline complet de transformation pour chacune des colonnes restantes du dataset (AgeuponOutcome,AnimalType,SexuponOutcome, Breed).\n",
    "\n",
    "Vous êtes **libres** de vos choix, mais vous devez les **justifer** colonne par colonne.\n",
    "Par exemple, vous pouvez choisir de combiner des colonnes entre elles, de séparer une colonne en plusieurs ou encore d'éliminer complètement une colonne si vous le justifiez correctement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La partie déjà prétraitée du dataset est chargée dans **X_train1** et **X_test1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = pd.read_csv(\"data/train_preprocessed.csv\")\n",
    "X_test1 = pd.read_csv(\"data/test_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Color</th>\n",
       "      <th>HasName</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.973624</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.421532</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.973624</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.471381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.868974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Color  HasName  Month  Day  Hour\n",
       "0  0.973624      1.0    2.0  1.0   3.0\n",
       "1 -1.421532      1.0   10.0  1.0   2.0\n",
       "2  0.973624      1.0    1.0  3.0   2.0\n",
       "3 -1.471381      0.0    7.0  1.0   3.0\n",
       "4 -0.868974      0.0   11.0  1.0   2.0"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le reste du dataset que vous devez traiter est:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns = [\"Color\",\"Name\",\"DateTime\"])\n",
    "X_test = X_test.drop(columns = [\"Color\",\"Name\",\"DateTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AnimalType</th>\n",
       "      <th>SexuponOutcome</th>\n",
       "      <th>AgeuponOutcome</th>\n",
       "      <th>Breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>1 year</td>\n",
       "      <td>Shetland Sheepdog Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cat</td>\n",
       "      <td>Spayed Female</td>\n",
       "      <td>1 year</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Pit Bull Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cat</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>3 weeks</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Lhasa Apso/Miniature Poodle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AnimalType SexuponOutcome AgeuponOutcome                        Breed\n",
       "0        Dog  Neutered Male         1 year        Shetland Sheepdog Mix\n",
       "1        Cat  Spayed Female         1 year       Domestic Shorthair Mix\n",
       "2        Dog  Neutered Male        2 years                 Pit Bull Mix\n",
       "3        Cat    Intact Male        3 weeks       Domestic Shorthair Mix\n",
       "4        Dog  Neutered Male        2 years  Lhasa Apso/Miniature Poodle"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Question 11: AgeuponOutcome (1 point)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12: AnimalType (1 point)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 13: SexuponOutcome (1 point)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 14: Breed (1 point)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "**Question 15: Complétez pipeline ci-dessous (4 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import TransformationWrapper\n",
    "from preprocessing import LabelEncoderP\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# pipeline_color = Pipeline([\n",
    "#     (\"name\", Transformer()),\n",
    "# ])\n",
    "\n",
    "\n",
    "# full_pipeline = ColumnTransformer([\n",
    "#         (\"color\", pipeline_color, [\"Color\"]),\n",
    "        \n",
    "\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lancez le pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_names = []\n",
    "# X_train_prepared = pd.DataFrame(full_pipeline.fit_transform(X_train),columns = columns)\n",
    "# X_test_prepared = pd.DataFrame(full_pipeline.fit_transform(X_test),columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concaténation des deux parties du dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = pd.concat([X_train1,X_train_prepared], axis = 1)\n",
    "# X_test = pd.concat([X_test1,X_test_prepared], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model selection (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encodage de la classe cible sous forme d'entiers pour l'utiliser\n",
    "avec les algorithmes de scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adoption' 'Died' 'Euthanasia' 'Return_to_owner' 'Transfer']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "target_label = LabelEncoder()\n",
    "y_train_label = target_label.fit_transform(y_train)\n",
    "print(target_label.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble de validation\n",
    "Pour comparer différents modèles entre eux, on ne peut pas utiliser\n",
    "l'ensemble de test, sinon on serait tenté de garder le modèle correspondant le mieux à l'ensemble de test ce qui pourrait conduire à l'overfitting.\n",
    "\n",
    "Il est d'usage de créer un nouvel ensemble de la taille de l'ensemble de test, l'ensemble de **validation**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "La cross-validation est une méthode utile pour comparer la performance de différents modèles de machine learning **sans créer d'ensemble de validation**.\n",
    "\n",
    "Il existe différents types de cross-validation, la procédure la plus classique est la suivante:\n",
    "* Diviser aléatoirement l'ensemble d'entraînement en deux parties (90%/10% par exemple).\n",
    "* Entraîner le modèle sur la plus grande partie, et le tester sur l'autre partie.\n",
    "* Recommencer n fois\n",
    "* Calculer la moyenne et l'écart type des résultats\n",
    "\n",
    "Les avantages sont les suivants:\n",
    "* Considérer la totalité de l'ensemble d'entraînement pour l'évaluation (sans se priver de l'ensemble de validation)\n",
    "* Obtenir l'écart-type des résultats permet une meilleure évaluation de la précision du modèle.\n",
    "\n",
    "L'inconvénient principal est le temps de calcul, étant donné que l'on effectue l'apprentissage du modèle plusieurs fois, cette méthode peut être impossible pour des datasets contenant un grand nombre d'exemple (> 10e5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 2: StratifiedKFold (1 point)\n",
    "\n",
    "En observant la distribution des classes de l'attribut cible (à l'aide des fonctions de visualisation de pandas), justifiez l'utilisation de l'objet **StratifiedKFold** de sklearn pour la division de l'ensemble d'entraînement lors de cross-validation en comparaison à une méthode pûrement **aléatoire**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 16: (1 point)\n",
    "\n",
    "\n",
    "**Choisir au moins deux modèles permettant la classification multiclasse sur sklearn en plus du modèle implémenté dans la première partie du TP**.\n",
    "\n",
    "**Complétez la fonction compare qui effectue la crossvalidation pour différents modèles et différentes métriques, et renvoie la liste des moyennes et écart-types pour chacune des métriques, pour chacun des modèles. **\n",
    "\n",
    "**En vous basant sur les différentes métriques, concluez quant au modèle le plus performant.**\n",
    "\n",
    "Evaluez les modèles pour les différentes métriques proposées:\n",
    "* **log loss**: c'est la métrique d'évaluation de kaggle\n",
    "* **precision**: correspond à la qualité de la prédiction, le nombre de classes correctement prédites par le nombre de prédiction total\n",
    "* **recall**: le nombre d'éléments appartenant à une classe, identifiés comme tel, divisé par le nombre total des éléments de cette classe.\n",
    "* **f-score**: une moyenne de la precision et du recall\n",
    "\n",
    "**Remarque: precision et recall sont deux mesures complémentaires pour l'évaluation d'un modèle de classification multi-classe.**\n",
    "\n",
    "Dans le cas d'une classification binaire avec un déséquilibre de la classe cible important, (90%/10%), en évaluant le résultat de la classification avec l'accuracy (nombre de prédictions correctes divisé par le nombre de prédictions total), on peut obtenir un très bon score (90% d'accuracy) en choisissant de prédire systématiquement la classe majoritaire.\n",
    "\n",
    "Dans un tel cas, la precision serait élevée de même, mais le recall serait très bas , nous indiquant la médiocrité de notre modèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(models,X_train,y_train,nb_runs):\n",
    "    losses = []\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SoftmaxClassifier import SoftmaxClassifier\n",
    "\n",
    "nb_run = 3\n",
    "\n",
    "models = [\n",
    "    SoftmaxClassifier(),\n",
    "]\n",
    "\n",
    "scoring = ['neg_log_loss', 'precision_macro','recall_macro','f1_macro']\n",
    "\n",
    "compare(models,X_train,y_train_label,nb_run,scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 17: Matrice de confusion (0.5 point)\n",
    "\n",
    "La matrice de confusion A est telle que $A_{i,j}$ correspond au nombre d'exemples de la classe i classifié comme appartenant à la classe j.\n",
    "\n",
    "Entrainez le modèle sélectionné sur la totalité de l'ensemble d'entraînement.\n",
    "A l'aide de la matrice de confusion et de la distribution des classes, analysez plus en détail les performances du modèle choisi et justifiez les."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train selected model\n",
    "\n",
    "selected_model = \n",
    "y_pred = selected_model.fit_predict(X_train,y_train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pd.DataFrame(confusion_matrix(y_train_label, y_pred), columns = target_label.classes_, index = target_label.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Distribution des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "print(target_label.classes_)\n",
    "pd.Series(y_train_label).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 3: Optimisation des hyper-paramètres (1 point)\n",
    "\n",
    "Les hyper-paramètres sont les paramètres fixés avant la phase d'apprentissage. Pour optimiser les performances du modèle, on peut sélectionner les meilleurs hyper-paramètres.\n",
    "\n",
    "A l'aide de sklearn, optimisez les hyper-paramètres du modèle que vous avez sélectionné et montrez que les performances ont été améliorées.\n",
    "Vous pouvez utiliser par exemple: **GridSearchCV**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 18: Soumission (0.5 point)\n",
    "\n",
    "Enfin, effectuez la prédiction sur l'ensemble de test et joignez vos résultats au rendu du TP.\n",
    "\n",
    "**Optionnel**: Vous pouvez soumettre vos résultats sur kaggle et noter votre performance en terme de log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = \n",
    "# pred_test = pd.Series(best_model.transform(X_test))\n",
    "# pred_test.to_csv(\"test_prediction.csv\",index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
